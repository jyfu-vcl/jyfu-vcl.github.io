---
title: "Enhanced Template-based Intra Mode Derivation with Adaptive Block Vector Replacement"
collection: publications
category: manuscripts
permalink: /publication/2025-TinySplat-number-4
excerpt: 'This paper introduces a V-PCC liked method to leverage x265 codec compress 3DGS-based data.'
date: 2025-12-01
venue: 'IEEE Transactions on Circuits and Systems for Video Technology (*Under Review*)'
paperurl: 'https://academicpages.github.io/files/TinySplat2025.pdf'
citation: 'Song, Z., Fu, J., Zhang, J., Lu, X., Jia, C., Ma, S. and Gao, W., 2025. TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation. arXiv preprint arXiv:2506.09479.'
---
The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a new paradigm to reconstruct 3D scenes. Using neural networks trained on large-scale multi-view datasets, it can directly infer 3DGS representations from sparse input views. Although the feedforward approach achieves high reconstruction speed, it still suffers from the substantial storage cost of 3D Gaussians. Existing 3DGS compression methods relying on scene-wise optimization are not applicable due to architectural incompatibilities. To overcome this limitation, we propose TinySplat, a complete feedforward approach for generating compact 3D scene representations. Built upon standard feedforward 3DGS methods, TinySplat integrates a trainingfree compression framework that systematically eliminates key sources of redundancy. Specifically, we introduce View-Projection Transformation (VPT) to reduce geometric redundancy by projecting geometric parameters into a more compact space. We further present Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy by aligning feature energy along dominant viewing directions via basis transformation. Lastly, spatial redundancy is addressed through an off-the-shelf video codec. Comprehensive experimental results on multiple benchmark datasets demonstrate that TinySplat achieves over 100Ã—compression for 3D Gaussian data generated by feedforward methods. Compared to the state-of-the-art compression approach, we achieve comparable quality with only 6% of the storage size. Meanwhile, our compression framework requires only 25% of the encoding time and 1% of the decoding time.
